{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Config\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import requests\n",
    "import multiprocessing\n",
    "#beautiful soup is a noteworthy API to try\n",
    "\n",
    "#SQL\n",
    "import pyodbc\n",
    "from sqlalchemy import event, create_engine\n",
    "from collections import defaultdict\n",
    "\n",
    "#for text preprocessing\n",
    "from string import digits\n",
    "#for building document term matrices\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#SEC forms scraper\n",
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "#nltk stopwords \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove string/defaultdict to check for relevant texts\n",
    "- Only need one date in scraper class init\n",
    "\n",
    "- Categorize prediction by company size. We can get the market cap data using factset, for those companies that are public.\n",
    "\n",
    "- Way to scrape 8K or 10K forms for public companies,  https://pypi.org/project/sec-edgar-downloader/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create google news object\n",
    "# googlenews = GoogleNews(start='10/08/2020',end='10/08/2020', lang ='en')\n",
    "\n",
    "# #search criterion\n",
    "# googlenews.search(\"apple news\")\n",
    "# #get page, can loop over all page but probably will obtain irrelevant info down the search\n",
    "# googlenews.getpage(1)\n",
    "\n",
    "# #obtain results for this current page. lists of dicts which stores information.\n",
    "# # date, title and desc are the key elements\n",
    "# res = googlenews.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #one result for the first page search\n",
    "# res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #obtain list all titles for this particular pages\n",
    "# titles = googlenews.gettext()\n",
    "# titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #before getting new results from other page, clear results\n",
    "# googlenews.clear()\n",
    "\n",
    "# #if this is not cleared, the googlenews objects keeps stores the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create a class to extract and store relevant articles with this API, and later feed them into NLP models. Let's try other APIs, like beautifulsoup. The idea was to use newspaper library to further scrape these URLs provided by GoogleNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #config stores parameters of query, use this to make runtime faster (e.g. we don't want to scrape images)\n",
    "# config = Config()\n",
    "# config.fetch_images = False #no need for images\n",
    "# config.memoize_articles = False #no need for article caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #arbitrary news article on wsj\n",
    "# url = 'https://www.wsj.com/articles/how-russia-today-skirts-high-tech-blockade-to-reach-u-s-readers-11602078094'\n",
    "# #For different language newspaper refer above table \n",
    "# article = Article(url, language=\"en\", config=config)\n",
    "# #'download' the article before obtaining some relevant properties\n",
    "# article.download()\n",
    "\n",
    "# # call 'parse' to extract relevant information, expensive operation\n",
    "# article.parse()\n",
    "# # #call 'nlp' to extract other features from the article, such as keywords or summary\n",
    "# # article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Article Summary:', article.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling complex company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #need to handle complex company names, can't just discriminate on whitespace\n",
    "# # #e.g. cable one \n",
    "# # #e.g. riverside bank\n",
    "\n",
    "# # #\"apple stock news\" will be assessed by relevance with \"apple\".\n",
    "# # #solution: default dictionary \n",
    "\n",
    "\n",
    "# #ideally store this in a class\n",
    "# dd = defaultdict(lambda: 1)\n",
    "# t = dd['apple stock news']\n",
    "# print(' '.join('apple stock news'.lower().split()[:t]))\n",
    "\n",
    "# dd['riverside bank'] = 2\n",
    "# t = dd['riverside bank']\n",
    "# print(' '.join('riverside bank stock news'.lower().split()[:t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just leave this as is for now\n",
    "dd = defaultdict(lambda: 1)\n",
    "\n",
    "def date_util(date):\n",
    "    \"\"\"converts datetime object to \n",
    "    string and vice versa\n",
    "    \n",
    "    Args:\n",
    "        date :: str or datetime object\n",
    "            if str, must be in MM/DD/YYYY format\n",
    "    \"\"\"\n",
    "    \n",
    "    if (type(date) == str):\n",
    "        return dt.datetime.strptime(date,\"%m/%d/%Y\")\n",
    "    \n",
    "    if (type(date) != str):\n",
    "        return date.strftime(\"%m/%d/%Y\")\n",
    "        \n",
    "class scraper(object):\n",
    "    \"\"\"scrapes relevant google articles, given a list of search terms.\n",
    "       Uses GoogleNews, will extend support for pyGoogleNews \n",
    "    \"\"\"\n",
    "    \n",
    "    today = dt.datetime.now().strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "    def __init__(self, date_from = today, date_to = today, search_terms = []):\n",
    "        \"\"\" class instantiator.\n",
    "        \n",
    "        Args:\n",
    "            date_from :: str\n",
    "                date string in format MM/DD/YYYY, will only parse \n",
    "                articles released on that date until date_to\n",
    "                \n",
    "            date_to :: str\n",
    "                date string in same format as above. articles\n",
    "                dated after this date will not be parsed.\n",
    "            \n",
    "            search_terms :: list(str)\n",
    "                list of search terms to parse on google. relevancy of each\n",
    "                article will be assessed via default dictionary. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.date_from = date_from\n",
    "        self.date_to = date_to\n",
    "        self.search_terms = search_terms\n",
    "        #to be stored in methods\n",
    "        self.search_info = None\n",
    "        self.data = None\n",
    "        self.df = None\n",
    "        self.final_df = pd.DataFrame()\n",
    "    \n",
    "        #dbs and date scraped, for the write_sql method\n",
    "        self.dbs = []\n",
    "        self.dates_scraped = set()\n",
    "        \n",
    "    def set_date(self, date):\n",
    "        \"\"\"Utility to function to change\n",
    "            the date class variable. Useful for scraping.\n",
    "            \n",
    "        Args:\n",
    "            date :: str\n",
    "                date string in format MM/DD/YYYY     \n",
    "        \"\"\"\n",
    "        \n",
    "        self.date_from, self.date_to = date, date\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_links(self, pages = 1):\n",
    "        \"\"\"obtains all relevant links from the search,\n",
    "            for each company.\n",
    "            \n",
    "        Args:\n",
    "            pages :: int\n",
    "                number of google pages to search resuts from\n",
    "                \n",
    "        Stores:\n",
    "            links :: dict(list[dict])\n",
    "                dictionaries of list, keys being search terms\n",
    "                and values being relevant information (e.g. URL)\n",
    "        \"\"\"\n",
    "        \n",
    "        gnews = GoogleNews(start=self.date_from, end=self.date_to)\n",
    "        links = {}\n",
    "        \n",
    "        #obtaining all the URLs\n",
    "        for s in self.search_terms:\n",
    "            gnews.search(s)\n",
    "            for p in range(1,pages+1):\n",
    "                gnews.getpage(p)\n",
    "                result = gnews.result() #stores values until cleared\n",
    "            \n",
    "            links[s] = result\n",
    "            gnews.clear()\n",
    "            \n",
    "        #removing irrelevant links\n",
    "        for s in self.search_terms:\n",
    "            tmp = []\n",
    "            num = dd[s] #number of relevant terms in search_terms\n",
    "            rel_str = ' '.join(s.lower().split()[:num]) #relevant string\n",
    "\n",
    "            for d in links[s]:\n",
    "                #selection criterion, e.g. if search term  \n",
    "                #is 'apple news', then want to subset based on 'apple' rather than 'apple news'\n",
    "                #--> filter with first word of each search term\n",
    "                if rel_str in d['desc'].lower(): \n",
    "                    tmp.append(d)   \n",
    "            links[s] = tmp \n",
    "        \n",
    "        self.search_info = links\n",
    "        \n",
    "        return None    \n",
    "    \n",
    "    def process_link(self, link = None, nlp = False):\n",
    "        \"\"\"processes the linksobtain by get_links() method, extracts\n",
    "            both the text and a summary of the article with newspaper package\n",
    "            \n",
    "        Args:\n",
    "            link :: str\n",
    "                URL of links stored in the dictionary returned by get_links()\n",
    "            nlp :: bool\n",
    "                Whether or not to perform nlp on the text of the link. This extracts\n",
    "                a summary of the text, but is a somewhat expensive operation.\n",
    "        \n",
    "        Returns:\n",
    "            article :: 'article' object\n",
    "                object that contains parsed properties for the link, such as\n",
    "                summary, text and date.\n",
    "        \"\"\"\n",
    "        \n",
    "        #parameters for the processing\n",
    "        config = Config()\n",
    "        config.fetch_images = False #no need for images\n",
    "        config.memoize_articles = False #no need for article caching\n",
    "        \n",
    "        try:\n",
    "            article = Article(link, language = \"en\", config = config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            if nlp:  \n",
    "                article.nlp() #extract summary as per the newspaper API\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "        return article  \n",
    "    \n",
    "    def store_data(self, search_info = None, nlp = False):\n",
    "        \"\"\" stores data for all links, for each in search term.\n",
    "            e.g. date, the summary, text...\n",
    "           \n",
    "            Args:\n",
    "                links :: dict(list[dicts])\n",
    "                    dictionary that containts URLs for each of our\n",
    "                    search terms, e.g. returned by get_links() method.\n",
    "                    \n",
    "                nlp :: bool\n",
    "                    Whether a summary was extracted in the process_links()\n",
    "                    method.\n",
    "                   \n",
    "            Stores:\n",
    "                res :: dict(list[dicts])\n",
    "                    dictionary that stores info for all our searches. Can be used\n",
    "                    to make DataFrame easily, and then upload to SQL database later.\n",
    "                    Info stored for each link: date|search_term|link|summary|text\n",
    "        \"\"\"\n",
    "        \n",
    "        if search_info is None:\n",
    "            search_info = self.search_info\n",
    "        \n",
    "        res = {} #will build df using a dictionary\n",
    "        \n",
    "        for s in self.search_terms: #iterate over search terms\n",
    "            res[s] = []\n",
    "            \n",
    "            #relevant string\n",
    "            num = dd[s] \n",
    "            rel_str = ' '.join(s.lower().split()[:num]) \n",
    "            \n",
    "            for info in search_info[s]: #iterate over links\n",
    "                tmp = {}\n",
    "                #only need one date assuming we run this class daily\n",
    "                tmp['date'] = self.date_to \n",
    "                tmp['core_search_term'] = rel_str #to handle keys appropriately\n",
    "                tmp['link'] = info['link'] \n",
    "                tmp['title'] = info['title']\n",
    "                \n",
    "                #process the link, use try clause in case failure to process\n",
    "                a = self.process_link(tmp['link'])\n",
    "                try:\n",
    "                    tmp['text'] = a.text #might need to narrow depending on length of text\n",
    "                    if nlp:\n",
    "                        try:\n",
    "                            tmp['summary'] = a.summary\n",
    "                        except:\n",
    "                            tmp['summary'] = None\n",
    "                except:\n",
    "                    tmp['text'] = None\n",
    "        \n",
    "                #store result\n",
    "                res[s].append(tmp)\n",
    "        \n",
    "        self.data = res\n",
    "        print('search data stored, {}'.format(self.date_to))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def make_df(self, res = None):\n",
    "        \"\"\" returns dataframe containing all relevant\n",
    "            results for the day, for all our searches.\n",
    "           \n",
    "            Args:\n",
    "                res :: dict(list[dicts])\n",
    "                    dictionary stored after calling self.store_data()\n",
    "                   \n",
    "            \n",
    "            Stores:\n",
    "                df :: DataFrame\n",
    "                    DataFrame of results for the given searches\n",
    "        \"\"\"\n",
    "        \n",
    "        self.get_links()\n",
    "        self.store_data()\n",
    "        \n",
    "        if res is None:\n",
    "            res = self.data\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        for s in self.search_terms:\n",
    "            tmp = pd.DataFrame(res[s])\n",
    "            df = df.append(tmp)\n",
    "         \n",
    "        df = df.reset_index()\n",
    "        df.drop(columns= ['index'], inplace=True)\n",
    "        self.df = df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def write_sql(self, db_name):\n",
    "        #change this, remove exists ect...\n",
    "        \"\"\"creates SQL table in internship server.\n",
    "           Will only work once make_df() has been called.\n",
    "        \n",
    "            Args:\n",
    "                db_name :: str\n",
    "                    name of the database\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.df is None:\n",
    "            print('no data: call make_df() first')\n",
    "            return None\n",
    "    \n",
    "        else:\n",
    "            server = 'mssql+pyodbc://internuser:internuser1@csidbdev/Internship?driver=SQL+Server+Native+Client+11.0'\n",
    "            sql_engine = create_engine(server)\n",
    "            \n",
    "            if db_name in self.dbs:\n",
    "                self.df.to_sql(db_name, sql_engine, if_exists = 'append')\n",
    "                print('appended to database {}'.format())\n",
    "               \n",
    "            else:\n",
    "                self.df.to_sql(db_name, sql_engine)\n",
    "                print('created new database {}'.format(db_name))\n",
    "                self.dbs.append(db_name)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def scrape_period(self, begin, end, SQL = False, db_name = None):\n",
    "        \"\"\"scrape news for entire period, from begin to\n",
    "        (including) end, for all search terms provided. stores \n",
    "        into SQL if needed.\n",
    "        \n",
    "        Args:\n",
    "            db_name :: str\n",
    "                name of the database to create/append to\n",
    "            \n",
    "            begin, end :: str\n",
    "                date string in format MM/DD/YYYY   \n",
    "                \n",
    "            SQL :: bool\n",
    "                whether to store into SQL as well\n",
    "        \"\"\"\n",
    "        \n",
    "        while date_util(begin) <= date_util(end):\n",
    "            if begin not in self.dates_scraped:\n",
    "        \n",
    "                #set date to scrape\n",
    "                self.set_date(begin)\n",
    "                \n",
    "                #obtain data\n",
    "                self.make_df()\n",
    "                \n",
    "                #store into SQL and class variable\n",
    "                if SQL:\n",
    "                    self.write_sql(db_name)\n",
    "                    \n",
    "                self.final_df = self.final_df.append(self.df)\n",
    "                \n",
    "                #add to scraped dates\n",
    "                self.dates_scraped.add(begin)\n",
    "                \n",
    "            #increment date by one\n",
    "            begin = date_util(date_util(begin) + dt.timedelta(days=1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_SEC_form(path, ticker, form_type = '8-K', after_date = None, before_date = None):\n",
    "        \"\"\"Downloads SEC forms to given path.\n",
    "            Forms then need to be parsed.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automated email scraping all press releases on set of portfolio companies\n",
    "#keywords, sent every week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#example - default date is today\n",
    "\n",
    "s = scraper(search_terms=['microsoft news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search data stored, 11/13/2020\n"
     ]
    }
   ],
   "source": [
    "s.make_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s.scrape_period(begin = '10/01/2020', end = '10/5/2020', SQL = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEC forms scraper\n",
    "from sec_edgar_downloader import Downloader\n",
    "dl = Downloader(\"/Users/edwardsulitzer/Cowen/NLP Project/SEC_forms\")\n",
    "dl.get(\"8-K\", \"AAPL\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore below\n",
    "#create 3 tables\n",
    "\n",
    "# date | URL | text | news key: unique for each news/URL(incremental increase), drop duplicates\n",
    "# search key | company name \n",
    "# news key | company key | relevance/stock score\n",
    "\n",
    "#get 1 month of data for a few companies in those formats\n",
    "#think about the actual ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company key df\n",
    "# search_keys = pd.DataFrame(df.search_term.unique())\n",
    "# search_keys = search_keys.reset_index()\n",
    "# search_keys.columns = ['search_key','search_term']\n",
    "# search_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_links = df[['date','link','text']]\n",
    "# unique_links = unique_links.drop_duplicates(subset=['link']) #remove any duplicated link\n",
    "# unique_links = unique_links.reset_index()\n",
    "# unique_links.columns = ['news_key','date','link','text']\n",
    "# unique_links.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = df.merge(search_keys, how = 'outer', on='search_term')\n",
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a mapping (both sides) \n",
    "#think about way to associate industry news to specific company - maybe need another table for this \n",
    "# search key | new key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
