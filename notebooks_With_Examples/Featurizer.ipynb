{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import gensim \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "#for text preprocessing\n",
    "from string import digits\n",
    "#for building document term matrices\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk stopwords \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to save/load pre-processed corpus to train our own word2vec into our class\n",
    "- add support for Glove and other word embeddings\n",
    "- use other tokenizers than gensim's one\n",
    "- Scraping/bucketizing by news source for the prediction task\n",
    "- Add SEC form scraping to scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added Word2Vec support\n",
    "#add GloVE\n",
    "#Look into NMF factorization\n",
    "#Look into how we can use tf-idf\n",
    "\n",
    "class Featurizer(object):\n",
    "    \"\"\"class to preprocess corpuses or text\n",
    "    and ultimately create textual features.\"\"\"\n",
    "    \n",
    "    def __init__(self, lang = 'english'):\n",
    "        \"\"\"class instantiator.\"\"\"\n",
    "        self.stopset = set(stopwords.words(lang))\n",
    "        self.corpus = set()\n",
    "        self.tfidf = None\n",
    "        #for dimensionality reduction\n",
    "        self.scaler = None\n",
    "        self.pca = None\n",
    "        #word2vec\n",
    "        self.w2v = None\n",
    "        \n",
    "    def preprocess(self, input_str):\n",
    "        \"\"\"method to preprocess articles.\n",
    "        removes punctuation, digits & stopwords\n",
    "        for the given input string.\n",
    "        \n",
    "        Args:\n",
    "            input_str :: str\n",
    "                text to be preprocessed\n",
    "\n",
    "        Returns:\n",
    "            preprocessed string. Also stores \n",
    "            the cleaned string into corpus class variable\n",
    "        \"\"\"\n",
    "\n",
    "        input_str = input_str.lower()\n",
    "\n",
    "        #fastest way to remove/replace characters in python\n",
    "        digits_table = str.maketrans('', '', string.digits)\n",
    "        punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "        #can add more tables for future reference\n",
    "\n",
    "        tables = [digits_table, punct_table]\n",
    "        for t in tables:\n",
    "            input_str = input_str.translate(t)\n",
    "\n",
    "        #handling stopwords\n",
    "        input_str = ' '.join([word for word in input_str.split() if word not in self.stopset])\n",
    "        \n",
    "        if input_str not in self.corpus:\n",
    "            self.corpus.add(input_str)\n",
    "        \n",
    "        return input_str\n",
    "    \n",
    "    def PCA_fit(self, X_train, n_components=0.9):\n",
    "        \"\"\"fits PCA on training sample. stores the PCA\n",
    "        object and appropriate scaler to use on the test set.\n",
    "        \n",
    "        Args:\n",
    "            X_train :: DataFrame or np.array\n",
    "                Training set, can be relatively large matrix\n",
    "                of chosen unigram features.\n",
    "            \n",
    "            n_components :: float \n",
    "                number of components needed to explain\n",
    "                n_components% of the variation in X_train\n",
    "                \n",
    "        Returns:\n",
    "            PCs :: np.array\n",
    "                principal components of the training set\n",
    "        \"\"\"\n",
    "        \n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        self.scaler = sc \n",
    "    \n",
    "        #obtaining the PCs from training data\n",
    "        pca_fit = PCA(n_components)\n",
    "        PCs = pca_fit.fit(X_train_std)\n",
    "        self.pca = pca_fit \n",
    "        \n",
    "        return PCs\n",
    "    \n",
    "    def PCA_transform(self, X_test):\n",
    "        \"\"\"transform out-of-sample examples\n",
    "        with PCs from training sample.\"\"\"\n",
    "        \n",
    "        if (self.scaler == None) or (self.pca == None):\n",
    "            return \"must fit PCA to the training sample first\"\n",
    "        \n",
    "        X_test_std = self.scaler.transform(X_test)\n",
    "        \n",
    "        return self.pca.transform(X_test_std)\n",
    "\n",
    "    def tfidf_fit(self, use_idf = True, max_df = 1.0, min_df = 1, max_features = None):\n",
    "        \"\"\"generates the document term frequency\n",
    "        matrix for the stored training corpus. Can serve as simple\n",
    "        NLP baseline for the regression task.\n",
    "        \n",
    "        Args:\n",
    "            use_idf :: bool\n",
    "                whether or not to use idf weights. if\n",
    "                set to False, simply uses tf weights\n",
    "                \n",
    "            max_df/min_df :: float or int\n",
    "                if in [0,1], represents proportion \n",
    "                of terms to ignore with respect to corpus \n",
    "\n",
    "            max_features :: int\n",
    "                considers only the \"max_features\" most frequent\n",
    "                terms, if specified\n",
    "                \n",
    "        Returns:\n",
    "            df :: DataFrame\n",
    "                document term frequency matrix for the corpus\n",
    "        \"\"\"\n",
    "        #params of the vectorizer\n",
    "        tfidf = TfidfVectorizer(lowercase = False, \n",
    "                        preprocessor = self.preprocess,\n",
    "                        stop_words = None,\n",
    "                        ngram_range=(1,1),\n",
    "                        tokenizer = None,\n",
    "                        max_df = max_df, min_df = min_df,\n",
    "                        max_features = max_features,\n",
    "                        use_idf = use_idf)\n",
    "        \n",
    "        features = tfidf.fit_transform(self.corpus)\n",
    "        self.tfidf = tfidf \n",
    "        df = pd.DataFrame(features.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def tfidf_transform(self, test_corpus):\n",
    "        \"\"\"transforms articles according to\n",
    "        trained tfidf model. Use on out-of-sample articles to obtain\n",
    "        textual tfidf features. \n",
    "        \n",
    "        Args:\n",
    "            test_corpus :: list(str)\n",
    "                list of articles to transform into tfidf features\n",
    "                \n",
    "        Returns:\n",
    "            X_test :: DataFrame\n",
    "                articles transformed into features\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.tfidf is None:\n",
    "            return 'Must fit an tfidf model on training corpus first'\n",
    "        \n",
    "        X_test = self.tfidf.transform(test_corpus)\n",
    "        X_test = pd.DataFrame(data=X_test.toarray(), columns= f.tfidf.get_feature_names())\n",
    "        \n",
    "        return X_test\n",
    "    \n",
    "    def word2vec(self, pre_train_path = None, size = 100, min_count = 1, window = 5):\n",
    "        \"\"\"loads or train Word2Vec embeddings \n",
    "        using class corpus.\n",
    "        \n",
    "        Args:\n",
    "            pre_train_path :: str\n",
    "                path to file containing pre-trained embeddings,\n",
    "                e.g. GoogleNews embeddings\n",
    "                \n",
    "            size :: int\n",
    "                dimension of the word embedding,\n",
    "                e.g. # of features\n",
    "        \n",
    "            min_count :: int\n",
    "                ignores words occuring less than min_count\n",
    "                \n",
    "        Stores:\n",
    "            self.w2v :: Word2Vec object\n",
    "        \"\"\"\n",
    "        \n",
    "        #using pre_trained embeddings\n",
    "        if pre_train_path:\n",
    "            pre_trained = KeyedVectors.load_word2vec_format(pre_train_path, binary=True)\n",
    "            \n",
    "            self.w2v = pre_trained \n",
    "            return 'stored pre-trained word embeddings'\n",
    "        \n",
    "        #tokenizing articles\n",
    "        tokenized_articles = []\n",
    "        for a in f.corpus:\n",
    "            #maybe use another tokenizer, not sure if gensim.utils is best\n",
    "            tokenized_articles.append(gensim.utils.simple_preprocess(a))\n",
    "        \n",
    "        model = Word2Vec(sentences = tokenized_articles, sg = 0, size = size, \n",
    "                         min_count = min_count, workers = -1, window = window)\n",
    "        self.w2v = model\n",
    "        \n",
    "        return 'trained word embeddings on corpus' \n",
    "    \n",
    "    def article2vec(self, article):\n",
    "        \"\"\"given an aticle/text,\n",
    "        returns a word embedding for that particular text\n",
    "        using the self.w2v model (obtained with word2vec).\n",
    "        \n",
    "        Args:\n",
    "            article :: str \n",
    "                article text\n",
    "                \n",
    "        Returns:\n",
    "            vec :: np.array\n",
    "                word embedding of the text\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.w2v is None:\n",
    "            return \"must load/train a word2vec model first\"\n",
    "        \n",
    "        vec = np.zeros(shape = (self.w2v.vector_size))\n",
    "    \n",
    "        txt = self.preprocess(article)\n",
    "        #convert into tokens, can perhaps be more picky here\n",
    "        tokens = gensim.utils.simple_preprocess(txt)\n",
    "        \n",
    "        for t in tokens:\n",
    "            #check if token in vocab\n",
    "            if t in self.w2v.wv:\n",
    "                #maybe consider using a weighted word2vec embedding\n",
    "                vec += self.w2v.wv[t]\n",
    "        \n",
    "        return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processed articles/text: {'vineet said concatenate articles', 'information document one', 'last article news', 'information second document'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>articles</th>\n",
       "      <th>concatenate</th>\n",
       "      <th>document</th>\n",
       "      <th>information</th>\n",
       "      <th>last</th>\n",
       "      <th>news</th>\n",
       "      <th>one</th>\n",
       "      <th>said</th>\n",
       "      <th>second</th>\n",
       "      <th>vineet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.667679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667679</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article  articles  concatenate  document  information     last     news  \\\n",
       "0  0.00000       0.5          0.5  0.000000     0.000000  0.00000  0.00000   \n",
       "1  0.00000       0.0          0.0  0.526405     0.526405  0.00000  0.00000   \n",
       "2  0.57735       0.0          0.0  0.000000     0.000000  0.57735  0.57735   \n",
       "3  0.00000       0.0          0.0  0.526405     0.526405  0.00000  0.00000   \n",
       "\n",
       "        one  said    second  vineet  \n",
       "0  0.000000   0.5  0.000000     0.5  \n",
       "1  0.667679   0.0  0.000000     0.0  \n",
       "2  0.000000   0.0  0.000000     0.0  \n",
       "3  0.000000   0.0  0.667679     0.0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example \n",
    "training_corpus = ['Vineet has said to concatenate articles', 'this is information about document one', \n",
    "                   '$3NoW, so!me^ information about the >,/?seco0nd! 488492document',\n",
    "                  'this is the last article news!!']\n",
    "f = Featurizer()\n",
    "for a in training_corpus:\n",
    "    f.preprocess(a)\n",
    "\n",
    "print('pre-processed articles/text:', f.corpus)\n",
    "print('')\n",
    "doc_matrix = f.tfidf_fit(use_idf = True)\n",
    "doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.PCA_fit(doc_matrix, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new testing examples to be used as features\n",
    "test_corpus = ['one test instance']\n",
    "\n",
    "test_matrix = f.tfidf_transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06117865,  2.20290661]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.PCA_transform(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'important test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of pre-processing\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "def preprocess(input_str):\n",
    "    \"\"\"removes punctuation and digits\n",
    "    for the given input string.\"\"\"\n",
    "    \n",
    "    input_str = input_str.lower()\n",
    "    \n",
    "    #fastest way to remove/replace characters in python\n",
    "    digits_table = str.maketrans('', '', string.digits)\n",
    "    punct_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    \n",
    "    #can add more tables for future reference\n",
    "\n",
    "    tables = [digits_table, punct_table]\n",
    "    for t in tables:\n",
    "        input_str = input_str.translate(t)\n",
    "\n",
    "    #handling stopwords\n",
    "    input_str = ' '.join([word for word in input_str.split() if word not in stopset])\n",
    "    \n",
    "    return input_str\n",
    "\n",
    "preprocess('This&/ $!!is a #42 {[impor^%tant]} +70TEST|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using PCA to reduce dimensionality. Maybe try kernel PCA?\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    stratify=y, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "#neeeds to perform scaling as per the training sample\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 0.9)\n",
    "#obtaining the principal components for training data\n",
    "PCs = pca.fit_transform(X_train_std)\n",
    "new_PC = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search data stored, 11/15/2020\n",
      "search data stored, 10/01/2020\n",
      "search data stored, 10/02/2020\n",
      "search data stored, 10/03/2020\n",
      "search data stored, 10/04/2020\n",
      "search data stored, 10/05/2020\n",
      "search data stored, 10/06/2020\n",
      "search data stored, 10/07/2020\n",
      "search data stored, 10/08/2020\n",
      "search data stored, 10/09/2020\n",
      "search data stored, 10/10/2020\n",
      "CPU times: user 50.7 s, sys: 1.13 s, total: 51.8 s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "#getting data to train word2vec model, 10 days of articles on apple\n",
    "%run Scraper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained word embeddings on corpus'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### using word2vec ####\n",
    "f = Featurizer()\n",
    "#preprocessing articles\n",
    "for t in s.final_df.text:\n",
    "    if t:\n",
    "        f.preprocess(t)\n",
    "        \n",
    "#sentences: list of lists of tokens. Can be changed to support n-grams as well.\n",
    "#sg: whether to use skipgram or cbow  (default cbow)\n",
    "#size: dimensionality of word space (e.g how many features to use)\n",
    "#min_count: ignores words that occur less than this. Can probably handle else where in pre-processing.\n",
    "#workers: how many cores to use\n",
    "\n",
    "f.word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sountrap', 0.36851558089256287),\n",
       " ('tamp', 0.32931700348854065),\n",
       " ('agreement', 0.3131377696990967),\n",
       " ('charts', 0.31090497970581055),\n",
       " ('administration', 0.30836647748947144),\n",
       " ('integral', 0.301155686378479),\n",
       " ('close', 0.2970578074455261),\n",
       " ('recognizable', 0.29581451416015625),\n",
       " ('read', 0.2942274808883667),\n",
       " ('filing', 0.2928212881088257)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtain similar words with cosine similarity\n",
    "#note that the result is pretty \"awful\" here (pun intended), since we trained on a very small corpus\n",
    "#to remedy this, use either larger corpus or load a pre-trained model\n",
    "f.w2v.wv.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example = s.df.text[0]\n",
    "# f.article2vec(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stored pre-trained word embeddings'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading pre-stored word2vec\n",
    "f.word2vec(pre_train_path ='PreTrained_Vecs.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the google word2vec model \n",
    "#issue is that distribution might differ from articles we are pulling our data from, hence the need to train our own\n",
    "# filename = 'PreTrained_Vecs.bin.gz'\n",
    "# # pre_trained = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "# # calculate: (king - man) + woman = ?\n",
    "# f.w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
